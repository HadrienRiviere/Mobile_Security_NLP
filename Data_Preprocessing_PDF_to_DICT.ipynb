{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the installation shell script in this section to install the required packages :\n",
    "# https://github.com/jsvine/pdfplumber for more explanations on how to use pdfplumber\n",
    "\n",
    "# !pip install pdfplumber\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK might require some installations : I may have forget some of them, but each error due to that should \n",
    "# advice a specific python statement such as the following to download the components needed. \n",
    "\n",
    "# nltk.download('words')\n",
    "\n",
    "# https://www.sandwichpdf.com/ : get text searchable pdf editer : input a pdf with text under image, and this will transform\n",
    "# the pdf into text searchable pdf file, to process it with the following scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to know what is inside the different folders\n",
    "def update_folder_contents():\n",
    "    manuals_pdf_folder = os.listdir(os.getcwd()+\"/manuals_pdf\")\n",
    "    pdf_files = [f for f in manuals_pdf_folder if f.endswith('.pdf')]\n",
    "    manuals_txt_folder = os.listdir(os.getcwd()+\"/manuals_txt\")\n",
    "    txt_files = [f for f in manuals_txt_folder if f.endswith('.txt')]\n",
    "    manuals_dict_folder = os.listdir(os.getcwd()+\"/manuals_dict\")\n",
    "    dict_files = [f for f in manuals_dict_folder if f.endswith('.txt')]\n",
    "    return pdf_files,txt_files,dict_files\n",
    "\n",
    "# Use this function to determine the PDF files that need their text to be extracted or the TXT files that must be transformed into dictionary\n",
    "def check_new_works():\n",
    "    pdf_files, txt_files, dict_files = update_folder_contents()\n",
    "    pdf_to_txt_needed, txt_to_dict_needed = [],[]\n",
    "    for pdf_file in pdf_files :\n",
    "         if os.path.splitext(pdf_file)[0]+'.txt' not in txt_files: \n",
    "            pdf_to_txt_needed.append(pdf_file)\n",
    "    for txt_file in txt_files : \n",
    "        if os.path.splitext(txt_file)[0]+'_dict.txt' not in dict_files: \n",
    "            txt_to_dict_needed.append(txt_file)\n",
    "    return pdf_to_txt_needed, txt_to_dict_needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pdf into into text files \n",
    "def pdf_to_txt(filename):\n",
    "    if os.path.splitext(filename)[0]+'.txt' not in txt_files: \n",
    "        with pdfplumber.open('manuals_pdf/'+filename) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                if page.extract_text():\n",
    "                    if text!='':\n",
    "                        text = text + ' ' + page.extract_text() \n",
    "                    else :\n",
    "                        text = text + page.extract_text()\n",
    "            if(text==''):\n",
    "                print(\"Report problem with pdf : \"+pdf_files[i])\n",
    "            with open('manuals_txt/'+os.path.splitext(filename)[0]+'.txt','w',encoding='utf-8') as filehandle:\n",
    "                filehandle.write(str(text))\n",
    "        print(\"Done : \" + str(filename))\n",
    "    else: \n",
    "        print(\"Already done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the txt files files we just created\n",
    "# Interest list of words\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "regex = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "# Custom this list for words to KEEP that are important for topic modelling tasks down the road\n",
    "# This custom list MUST include : the words that we want to keep, as well as STEMS of words we want to keep\n",
    "# Run the following section with the specific word to understand whether this is useful or not to add the real word\n",
    "# or its stem or lemma. \n",
    "custom_list = ['app','application','download','mail','email','bluetooth','pair','installation','setup','guide','howto','QR',\n",
    "               \"wifi\",\"Wi-fi\",'customize','QR-code','phone','smartphone','website','site','web','desktop','installing',\n",
    "              'detects','device','devices','username','creating','syncs','downloading','restarts','storing','internet','instal']\n",
    "custom_stop_list = ['m','rain','may','w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires is an english word : False\n",
      "requires porter's stem is requir ,which is an english word : False\n",
      "requires lemmatized word is requires ,which is an english word : False\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "word_test = \"requires\"\n",
    "porter_test = porter.stem(word_test)\n",
    "lemma_test = lemma.lemmatize(word_test)\n",
    "print(word_test + \" is an english word : \"+str(word_test in words.words()))\n",
    "print(word_test + \" porter's stem is \" + porter_test + \" ,which is an english word : \"+str(porter_test in words.words()))\n",
    "print(word_test + \" lemmatized word is \" + lemma_test + \" ,which is an english word : \"+str(lemma_test in words.words()))\n",
    "\n",
    "# If at least one of the test is True, then the word will be kept within the dictionary corresponding to the specific PDF file\n",
    "\n",
    "# If all tests are False, we may consider adding the porter or lemma root to the custom list if the word can be changed\n",
    "# We had download, because the porter root will redirect to download the following \"downloading\",\"downloaded\",\"downloads\"\n",
    "# This is not required for names that won't be changing such as firstname, or product names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words (\"the\",\"a\",\"in\"...), and puntuation\n",
    "def clean(doc):\n",
    "    doc = re.sub(r'\\d+', '', doc)\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if (i not in stop) and (i not in custom_stop_list)])\n",
    "    normalized = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    return normalized\n",
    "\n",
    "# Keep the english words recognized in the dictionary: might miss some words, that is why we keep track of the deleted words\n",
    "def english_words_only(doc):\n",
    "    returned_doc, deleted_doc = [],[]\n",
    "    print(\"The doc is of length :\"+str(len(doc)))\n",
    "    for word in doc:\n",
    "        if word not in deleted_doc:\n",
    "            if word in words.words() or word in custom_list:\n",
    "                returned_doc.append(word)\n",
    "            else:\n",
    "                new_word = porter.stem(word)\n",
    "                if new_word in words.words() or new_word in custom_list:\n",
    "                    returned_doc.append(new_word)\n",
    "                else :\n",
    "                    new_word = lemma.lemmatize(word)\n",
    "                    if new_word in words.words() or new_word in custom_list:\n",
    "                        returned_doc.append(new_word)\n",
    "                    else:\n",
    "                        deleted_doc.append(word)\n",
    "    return returned_doc,deleted_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform a TXT file to a dictionary using the clean functions\n",
    "def txt_to_dict(filename):\n",
    "    new_text = ''\n",
    "    with open('manuals_txt/'+filename,'r',encoding='utf-8') as filehandle:\n",
    "        text_retrieved = filehandle.readlines()\n",
    "        text_retrieved = [item.replace(\"\\n\", \" \") for item in text_retrieved]\n",
    "        new_text = ' '.join(text_retrieved)\n",
    "    cleaned = clean(new_text).split(' ')\n",
    "    super_cleaned,removed = english_words_only(cleaned)\n",
    "    print(len(super_cleaned),len(removed))\n",
    "    with open('manuals_dict/'+os.path.splitext(filename)[0]+'_dict.txt','w',encoding='utf-8') as filehandle:\n",
    "                filehandle.write(str(super_cleaned))\n",
    "    with open('manuals_removed/'+os.path.splitext(filename)[0]+'_removed.txt','w',encoding='utf-8') as filehandle:\n",
    "                filehandle.write(str(removed))\n",
    "    print(\"Done : \" + str(filename))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at all the words that has been removed from the previous clean functions \n",
    "# If some words were removed and should not, may consider increasing the custom_list\n",
    "def reunite_removed():\n",
    "    manuals_removed_folder = os.listdir(os.getcwd()+\"/manuals_removed\")\n",
    "    removed_files = [f for f in manuals_removed_folder if f.endswith('.txt')]\n",
    "    all_deleted = []\n",
    "    total_len = 0\n",
    "    for removed_file in removed_files:\n",
    "        with open('manuals_removed/'+removed_file,'r',encoding='utf-8') as filehandle:\n",
    "            deleted = eval(filehandle.readline())\n",
    "            total_len = total_len + len(deleted)\n",
    "        for deleted_word in deleted:\n",
    "            if deleted_word not in all_deleted:\n",
    "                all_deleted.append(deleted_word)\n",
    "    with open('all_removed_words.txt','w',encoding='utf-8') as filehandle:\n",
    "                filehandle.write(str(all_deleted))\n",
    "    print(total_len,len(all_deleted))\n",
    "    print(all_deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section to convert all the NEW PDF files added in manuals_pdf that are not in format txt\n",
    "# Will look at the .pdf that do not exist in .txt\n",
    "pdf_to_txt_needed, txt_to_dict_needed = check_new_works()\n",
    "pdf_files, txt_files, dict_files = update_folder_contents()\n",
    "\n",
    "# Run the conversion for each\n",
    "for pdf_needed in pdf_to_txt_needed:\n",
    "    pdf_to_txt(pdf_needed)\n",
    "    \n",
    "# Check the new works to do : no pdf_to_txt_needed should appear\n",
    "pdf_to_txt_needed, txt_to_dict_needed = check_new_works()\n",
    "pdf_files, txt_files, dict_files = update_folder_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this section to convert all the NEW TXT files added in manuals_txt that are not converted to dictionary\n",
    "# Be careful this section might be time consuming, especially with long txt files, due to the use of NLTK libraries\n",
    "\n",
    "pdf_to_txt_needed, txt_to_dict_needed = check_new_works()\n",
    "pdf_files, txt_files, dict_files = update_folder_contents()\n",
    "\n",
    "# If we need to test only a few amount of files, consider reducing the size of txt_to_dict_needed, and then check the \n",
    "# removed words.\n",
    "\n",
    "# txt_to_dict_needed = txt_to_dict_needed[:10]\n",
    "# txt_to_dict_needed = ['amazon_echo_1st_gen.txt','Amazon_Echo_Dot_3rdGen_QSG_US_sw.txt','apple_tv_4thgen_sw.txt',\n",
    "#                      'August_coorbell_cam_sw.txt','Belkin_wemo_switch.txt']\n",
    "print(str(len(txt_to_dict_needed))+' Documents ready for dict transformation !')\n",
    "for txt_needed in txt_to_dict_needed:\n",
    "    txt_to_dict(txt_needed)\n",
    "    \n",
    "pdf_to_txt_needed, txt_to_dict_needed = check_new_works()\n",
    "pdf_files, txt_files, dict_files = update_folder_contents()\n",
    "\n",
    "reunite_removed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

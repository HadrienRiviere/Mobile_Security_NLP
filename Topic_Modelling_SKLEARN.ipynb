{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use SKLEARN library to model topic modelling with the dictionaries created\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to know what is inside the different folders\n",
    "def update_folder_contents():\n",
    "    manuals_pdf_folder = os.listdir(os.getcwd()+\"/manuals_pdf\")\n",
    "    pdf_files = [f for f in manuals_pdf_folder if f.endswith('.pdf')]\n",
    "    manuals_txt_folder = os.listdir(os.getcwd()+\"/manuals_txt\")\n",
    "    txt_files = [f for f in manuals_txt_folder if f.endswith('.txt')]\n",
    "    manuals_dict_folder = os.listdir(os.getcwd()+\"/manuals_dict\")\n",
    "    dict_files = [f for f in manuals_dict_folder if f.endswith('.txt')]\n",
    "    return pdf_files,txt_files,dict_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix of size : 68,5029\n"
     ]
    }
   ],
   "source": [
    "# We need to build out doc_term_matrix : column are number of words, and rows are for a specific topic, \n",
    "# doc_term_matrix(j,i) is the occurences of a word i within the document j \n",
    "pdf_files, txt_files, dict_files = update_folder_contents()\n",
    "my_words = []\n",
    "for dict_file in dict_files : \n",
    "    with open('manuals_dict/'+dict_file,'r',encoding='utf-8') as filehandle:\n",
    "        local_dict = eval(filehandle.readline())\n",
    "        for word in local_dict: \n",
    "            if word not in my_words:\n",
    "                my_words.append(word)\n",
    "print(\"Matrix of size : \"+str(len(dict_files))+\",\"+str(len(my_words)))\n",
    "doc_term_matrix = np.zeros((len(dict_files),len(my_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1786 not_used words\n",
      "There are 10 over_used words\n"
     ]
    }
   ],
   "source": [
    "# We should remove the words that appear in less than 2 documents\n",
    "# We should remove the words that appear in more that 80% of the documents\n",
    "topic_index = 0\n",
    "for dict_file in dict_files : \n",
    "    with open('manuals_dict/'+dict_file,'r',encoding='utf-8') as filehandle:\n",
    "        local_dict = eval(filehandle.readline())\n",
    "        for word in local_dict: \n",
    "            word_index = my_words.index(word)\n",
    "            doc_term_matrix[topic_index][word_index] += 1\n",
    "    topic_index += 1\n",
    "# Let's see in how many topics a specific word appears : stored in words_appearance\n",
    "words_appearance = []\n",
    "for i in range(len(my_words)):\n",
    "    words_appearance.append(0)\n",
    "    for j in range (len(dict_files)):\n",
    "        if doc_term_matrix[j][i] != 0:\n",
    "            words_appearance[i] += 1\n",
    "# How many words are not_used, how many are over_used ?\n",
    "not_used = 0\n",
    "over_used = 0\n",
    "for word_cnt in words_appearance:\n",
    "    if word_cnt < 2:\n",
    "        not_used += 1\n",
    "    elif word_cnt > 0.8*len(dict_files):\n",
    "        over_used += 1 \n",
    "print(\"There are \"+str(not_used)+\" not_used words\")\n",
    "print(\"There are \"+str(over_used)+\" over_used words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the new matrix of occurencies : (68, 3233)\n",
      "Size of the former matrix of occurencies : (68, 5029)\n"
     ]
    }
   ],
   "source": [
    "# Reduce the size of matrix by deleting the column (words) that are not used or over used : improves the learning\n",
    "doc_term_matrix_2 = doc_term_matrix\n",
    "for i in range(len(words_appearance)):\n",
    "    if (words_appearance[len(words_appearance)-i-1] < 2 or words_appearance[len(words_appearance)-i-1] > 0.8*len(dict_files)):\n",
    "        doc_term_matrix_2 = np.delete(doc_term_matrix_2,len(words_appearance)-i-1,axis=1)\n",
    "print(\"Size of the new matrix of occurencies : \"+str(doc_term_matrix_2.shape))\n",
    "print(\"Size of the former matrix of occurencies : \"+str(doc_term_matrix.shape))\n",
    "# We are now ready to use topic modelling tools !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=2, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the objct LDA as a topic modelling tool : n_components = 2 because we want two topics\n",
    "LDA = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "LDA.fit(doc_term_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['included', 'full', 'indic', 'briefing', 'power', 'must', 'new', 'user', 'storage', 'led', 'unusable', 'below', 'icon', 'prompt', 'search', 'achievement', 'white', 'buy', 'unmatched', 'generation']\n",
      "['restarts', 'content', 'screen', 'm', 'game', 'white', 'adapter', 'must', 'devices', 'central', 'calling', 'trigger', 'prompt', 'repeat', 'package', 'taa', 'user', 'cant', 'restrict', 'preview']\n"
     ]
    }
   ],
   "source": [
    "# let's havea look at the 20 most probable words for each topics : this will indicate how the topic selection is done\n",
    "# If the lists are not consistent with the objective, then we went wrong on sorting the documents\n",
    "for i in range (len(LDA.components_)):\n",
    "    topic_words = LDA.components_[i]\n",
    "    top_topic_word = topic_words.argsort()[-20:]\n",
    "    top_20_words = []\n",
    "    for num in top_topic_word:\n",
    "        top_20_words.append(my_words[num])\n",
    "    print(top_20_words)\n",
    "    \n",
    "# As we may see the topics are not consistent at all with our goal\n",
    "\n",
    "# First, we may want to use a custom stop word, to avoid specific words that are not significant on clustering the topics\n",
    "# See the Data_preprocessing_PDF_to_DICT notebook\n",
    "\n",
    "# Or we may want to use pdf that are only including the setup and installation instructions and NO OTHERS\n",
    "# I decided to try out a second test using only the pdf that include those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=2, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf = NMF(n_components=2, random_state=42)\n",
    "nmf.fit(doc_term_matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence', 'calling', 'generation', 'storage', 'content', 'single', 'search', 'regulatory', 'white', 'user', 'override', 'video', 'wired', 'devices', 'ay', 'premium', 'unplugging', 'taa', 'restrict', 'cant']\n",
      "['link', 'mainly', 'must', 'android', 'calling', 'adapter', 'led', 'search', 'white', 'pole', 'unresponsive', 'unusable', 'buy', 'neutral', 'august', 'icon', 'trigger', 'central', 'package', 'prompt']\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(nmf.components_)):\n",
    "    topic_words = nmf.components_[i]\n",
    "    top_topic_word = topic_words.argsort()[-20:]\n",
    "    top_20_words_2 = []\n",
    "    for num in top_topic_word:\n",
    "        top_20_words_2.append(my_words[num])\n",
    "    print(top_20_words_2)\n",
    "    \n",
    "# The vocabulary in associated with the topics are now a little more consistent with our goal.\n",
    "# We might then think that sorting out the PDF, only including the setup and installation part might be required\n",
    "# Plus, that will allow faster computations, and better learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
